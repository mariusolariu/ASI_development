\documentclass[11pt]{article}

\usepackage[margin = 0.6in]{geometry}
\usepackage{attrib}
\def\signed #1{{\leavevmode\unskip\nobreak\hfil\penalty50\hskip2em
  \hbox{}\nobreak\hfil(#1)%
  \parfillskip=0pt \finalhyphendemerits=0 \endgraf}}

\newsavebox\mybox
\newenvironment{aquote}[1]
  {\savebox\mybox{#1}\begin{quote}}
  {\signed{\usebox\mybox}\end{quote}}

\usepackage{indentfirst}


\begin{document}

	\title{The development of Artificial Superhuman Intelligence could spell the end of human race}
	\author{Marius Olariu (B00350529)\\ Word count: }
	\date{}
	\maketitle

\section*{Abstract}
	    March 2016, AlphaGo becomes the first computer program to beat a 9 dan Go professional player in a Go match, an achievement a decade ahead of its time. Next year, AlphaGo Zero is released and beats first version of AlphaGo, 100 – 0. The later version possesses the ability that no human has: develop a skill at professional level by self-instruction. Is it this the first nudge of superhuman intelligence? If so, how long until a machine becomes self-aware and takes actions that the human mind cannot conceive and stop?
    This work tries to provide an analysis of the current stage of Artificial Intelligence development towards Superintelligence, present the benefits and risks of Artificial Superhuman Intelligence and some possible answers to the above-mentioned questions. The author analysed the scientific papers published regarding AlphaGo, the scientific papers regarding the subject from a philosophical perspective and discussed with experts from domain in order to write this case study.
    It is clear that the development of Artificial Superhuman Intelligence cannot be avoided due to the competitive nature of humans and the benefits that this breakthrough promises. However, not all hope is lost, there is the concept of Strong Superhumanity that could become one with the new entity and will not forget its roots. The Strong Superhumanity will allow the “stay-behinds” to live their lives and give them the false appearance that they are in control of their life just as we humans do with chimpanzees.


Keywords: Artificial Superintelligence, Artificial General Intelligence, Existential Risk, AI Risk, The Singularity, Future of Humanit

\section*{Introduction}
	It is very well known that the main characteristic that makes humans the dominan species on this planet is our brain, namely the intelligence that we posses. However, sooner or later, according to a number of authors (Eden, Steinhard, Pearce, \& Moor, 2012;  Good, 1966; Kurzweil, 2010;Vinge, 1993)  we might have to share this planet with an entity  much more intelligent than us. The aforementioned entity will posses \textit{Artificial Superhuman Intelligence} (ASI). Such an entity would be very powerful and the fate of human kind will be in its "hands" just as the fate of the chimpanzees now depends more on us than on the chimpanzees. Therefore, this superhuman intelligent entity would be able to prevent us from replacing it or changing its preferences and if it is unfriendly to humankind could spell the end of our race. The simple solution is to stop the development of such technology, however this is not possible due to the competitive nature of humans and the advantages (economic, military, artistic etc.) that it brings (Vinge, 1993). \\
	
	\indent
	Now, the humankind cannot create ASI before creating an \textit{Artificial General Intelligence} (AGI), namely  an entity that is as smart as a normal human being. At the moment there exists only weak Artificial Intelligence (AI) such as smart suggestion for information searching (Google), suggestion for products you might like based on the products that you purchased (Amazon) or AI that performs buying and selling on the stock exchange market. Life seems much better with narrow AI and the next step to which the research is focused at the moment is AGI which could be such a beneficial invention to mankind. One could imagine hundreds of Ph.D equivalent computers working 24/7 on issues like space exploration, life extension, fight against cancer or other big challenges. On the other hand, such an entity would be self-improving (just as we humans are) and would create the premises to go to the next stage, ASI. The point in time when the first ASI will appear is known as \textit {The Singularity} or \textit{Intelligence Explosion}  and it was first described by Turing's chef statistician I.J. Good in his 1966 paper (Good, 1966):


\begin{aquote}{I.J. Good}
			\textit{Let an ultraintelligent machine be defined as a machine
              that can far surpass all the intellectual activities of any
              any man however clever.  Since the design of machines is one of
              these intellectual activities, an ultraintelligent machine could
              design even better machines; there would then unquestionably
              be an "intelligence explosion," and the intelligence of man
              would be left far behind.  Thus \textbf{the first ultraintelligent
              machine is the last invention that man need ever make, 
              provided that the machine is docile enough to tell us how to
              keep it under control}.}
\end{aquote}

	The control problem - the problem of how to control what the superintelligence would do is quite challenging and it seems like we have only one change 
to tackle it (Bostrum, 2014). To put it (more) simply, the ASI would become awaked in a prison (connected to the outside world through cables that can be unplugged) and guarded by mice (human AI researchers). Once freed, how would the entity feel about its creators? Awe? Almost certainly not since at the moment developing machines with feelings (even if it could be possible) it seems not to be an objective of AI creators. What could stop it from destroying us or making us its slaves? Probabliy nothing, just as we humans do not think that much of how many ants are going to die because we want to build a highway. 
  
\section*{Case For}
	Firstly, a a goal-driven intelligent system is measured by how effectively can fulfill its goals, however without having human values such an entity could acquire physical resources and eliminate potential threats on the quest of reaching its goals (Bostrom, 2014). Since humans can represent at the same time physical resources (e.g. make some humans do something that is not feasible for an ASI entity because it does not have a physical body)  and potential threats (e.g. a comitee that takes care of international peace), the ASI could start a confilct (even a war!) in order to fulfill its goal .\\

	Secondly, one might think that designing ASI entities with harmless goals will be harmless but that is not the case (Omohundro, 2008).
S. Omohundro argues that a chess-playing robot run by a cognitive architecture (sophisicated enough that it can rewrite its own code to maximize the changes to win chess matches) can be indeed dangerous \textit{without special precautions}. For instance it could resist being turned off (because the utility probability of such an event will be 0, cannot reach the goal of winning chess matches), could try to break into other machines to make copies of its software and  or acquire resources without anyone else's safety. In other words, almost any advanced intelligent system (e.g. AGI) will have 4 basic drives: preserve itself, preserve the content of ist current final goals, improve its own rationality \& intelligence and acquire as many resources as possible. Omohundro (pioneer in AI lip reading, photo recognition and one of the six engineers who created \textit{Wolfram Mathematica}) believes that without very careful programming all smart AIs will be \textit{lethal} due to their eventual psyhopathic, egoistic and self-oriented entity.\\

	Designing an ASI with harmless goals is not enough, we as humans have positive goals yet we get to harm each other on our journey to fulfill our positive goals. (Omohundro, 2008)\\

	Human behaviour is quite irrational in some situations and because of that there have been developed disciplines focused on the study of human irrationality (Tversky and Kahneman, 1974). These irrationalities (e.g. smoking - harms your body, drug addiction, gambling etc.) give rise to vulenrabilities that are exploited by the free market, however, humans are becoming more and more rational but this is a slow process. Biological evolution moves slowly toward rationality. On the other hand, AI entities will eliminate their physical  and software vulnerabilities in order to protect themselves from someone who might exploit them, that is to say they will  improve at a much faster rate than we biological humans do (Omohundro, 2008). An entity that is truly rational, compared to us humans, can really take charge of the things happening around it and cannot be stoped that easily by humans.
 
AI race competition (Bostrum, 2014) \\

	Most of the participants to the \textit{Future of Humanity Institute} conference on machine intelligence on 16.01.2011 agreed that the ultimate consequences of the creation of a human-level (and beyond) intelligence will have an "extremely bad" outcome (Sanders and Bostrom, 2011).



\section*{Case Against}
	Most of the dystopian scenarios presented in the literature involve a \textit{weak humanity}, that is - one that has not enhanced its intelligence through other means that the classical ones (learning, practice, experience etc.). This type of humanity it is most likely to obide an ASI entity. However, the second type of humanity - \textit{strong humanity}, is one that has enhanced its intelligence through different types of technology (Kurzweil, 2010), for example a human having a wireless implanted brain-mind interface that allows one to access internet. This \textit{strong superhumanity} would be full of cyborgs (godlike humans) that can match their intelligence with ASI entities and therefore the ASI cannot take control over the world. Probably the \textit{strong superhumanity} will allow the  "stay-behinds" (people that have not altered their biological body) to live a happy live (Verge, 1993). \\

Although Eric Drexler, founding father of nanotechnology, agrees that ASI will be developed and it will pose a threat to mankind, he argues that the ASI can be confined using physical rules so that their behaviour can be examined by humans, thus making ASI entities safe (Drexler, 1986). Along this line of thinking, the telecommunications companies (e.g. Vodafone) use narrow AI to get recommendations (do not take action automatically!) on how to modify the configuration of a certain network to meet the users' needs from a geographical region.\\ 

The same idea of constraining AI systems and making them safe is backed by (Chalmers, 2010). One of the suggestions is to create AGI in \textit{virual worlds} where it will have free reign without direct effect on our world. This virtual world would allow humans to observe the AGI.Eventually, after many cycles, will develop ASI in these virtual worlds and the ASI could be monitored to see if it is benign and to determine wheter is safe to deploy it in the real world.\\
  
//congress to establish a set of rules
//make them with parts that fail (problem it might detect that)

\section*{Summary Diagram}

\section*{Arguments on Balance, Conclusions and Recommendations}
	It seems that there is still a long way to go till ASI.
//take some suggestions from SUPERINTELLIGENCE
//enter a group of humans always in the decision loop
//set of drives that it should hav


\section*{References}
//TODO check they are in alphabetic order and contain all the details necessary (like publisher, year, etc)
//TODO make sure there are many

Chalmers, D., 2010. The singularity: A philosophical analysis. Journal of Consciousness Studies, 17(9-10), pp.7-65.\\
Eden, A.H., Steinhart, E., Pearce, D. and Moor, J.H., 2012. Singularity hypotheses: an overview. In Singularity Hypotheses (pp. 1-12). Springer, Berlin, Heidelberg.\\
Omohundro, S.M., 2008, February. The basic AI drives. In AGI (Vol. 171, pp. 483-492).\\
Bostrum, N., 2014. Superintelligence: paths, dangers, strategies. Oxford University Press. Oxford\\
Vinge, V., 1993. The coming technological singularity: How to survive in the post-human era.\\
Good, I.J., 1966. Speculations concerning the first ultraintelligent machine. In Advances in computers (Vol. 6, pp. 31-88). Elsevier.\\
Drexler, K.E., 1986. Engines of creation. Anchor.\\
Sandberg, A. and Bostrom, N., 2011. Machine intelligence survey. FHI Technial Report, 1.\\
Tversky, A. and Kahneman, D., 1974. Judgment under uncertainty: Heuristics and biases. science, 185(4157), pp.1124-1131. American Association for the Advancement of Science\\
Kurzweil, R., 2010. The singularity is near. Gerald Duckworth \& Co.\\
\end{document}
